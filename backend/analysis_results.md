# 🔍 Repository Insights

## 📃 Project Details 
- **Name:** local-LLM-with-RAG
- **Description:** Running local Language Language Models (LLM) to perform Retrieval-Augmented Generation (RAG)
- **Primary Language:** Python

## 🤓 Project Understanding 
<details>
  <summary><strong>Peek Under the Hood 👀</strong></summary>

  The repository **local-LLM-with-RAG** is structured to facilitate the development and testing of a Retrieval-Augmented Generation (RAG) system using local Large Language Models (LLMs). Below is a concise analysis of its file structure, main components, tech stack, and architecture:

### Main Components:
1. **Core Scripts:**
   - `app.py`: The main application script that handles the execution of the RAG system, including loading documents, generating embeddings, and querying.
   - `document_loader.py`: Responsible for loading and processing documents (e.g., PDFs) to be used in the RAG system.
   - `llm.py`: Likely contains functions or classes related to interacting with the local LLM.
   - `models.py`: Presumably defines the models used in the application, including LLMs and embedding models.

2. **User Interface:**
   - `ui.py`: Contains the Streamlit application code, providing a web interface for users to interact with the RAG system.

3. **Data and Resources:**
   - `Research/`: A directory containing sample PDF documents used for testing and demonstrating the RAG capabilities.
   - `images/`: Contains images used in the README and possibly within the UI, enhancing visual understanding.

4. **Configuration and Dependency Management:**
   - `pyproject.toml`: Specifies project dependencies and configurations, likely for package management.
   - `pyrightconfig.json`: Configuration file for Pyright, a type checker for Python, ensuring type safety in the codebase.
   - `uv.lock`: A lock file generated by the UV package manager, ensuring consistent installations of dependencies.

5. **Documentation:**
   - `README.md`: Provides an overview of the project, setup instructions, and usage guidelines.

### Tech Stack:
- **Python**: The primary programming language used for the application.
- **Ollama**: For running local LLMs.
- **Langchain**: A library for working with LLMs.
- **Chroma**: A vector database for managing embeddings.
- **Streamlit**: For creating the web UI.
- **PyPDF**: To handle PDF document processing.
- **UV**: A package installer for managing Python dependencies.

### Architecture:
The architecture revolves around a command-line and web-based interface for interacting with LLMs. The application loads documents, generates embeddings, and allows users to query these embeddings to retrieve relevant information. The use of Streamlit enhances user interaction, making it more intuitive compared to a purely command-line approach. The project is designed for experimentation, with a focus on testing various models and configurations for RAG.

Overall, this repository serves as a sandbox for developers interested in exploring the integration of local LLMs with retrieval-augmented generation techniques.

</details>

## 🌲 Project Structure 
<details>
  <summary><strong>File Tree</strong></summary>

  📁 images/
&nbsp;&nbsp;&nbsp;&nbsp;📄 [streamlit_ui.png](https://github.com/amscotti/local-LLM-with-RAG/blob/main/images/streamlit_ui.png)
&nbsp;&nbsp;&nbsp;&nbsp;📄 [wizard_experimenting.jpg](https://github.com/amscotti/local-LLM-with-RAG/blob/main/images/wizard_experimenting.jpg)
📁 Research/
&nbsp;&nbsp;&nbsp;&nbsp;📄 [2304.03442v1.pdf](https://github.com/amscotti/local-LLM-with-RAG/blob/main/Research/2304.03442v1.pdf)
&nbsp;&nbsp;&nbsp;&nbsp;📄 [2305.14325.pdf](https://github.com/amscotti/local-LLM-with-RAG/blob/main/Research/2305.14325.pdf)
&nbsp;&nbsp;&nbsp;&nbsp;📄 [2308.10848.pdf](https://github.com/amscotti/local-LLM-with-RAG/blob/main/Research/2308.10848.pdf)
&nbsp;&nbsp;&nbsp;&nbsp;📄 [2309.14391.pdf](https://github.com/amscotti/local-LLM-with-RAG/blob/main/Research/2309.14391.pdf)
📄 [.gitignore](https://github.com/amscotti/local-LLM-with-RAG/blob/main/.gitignore)
📄 [app.py](https://github.com/amscotti/local-LLM-with-RAG/blob/main/app.py)
📄 [document_loader.py](https://github.com/amscotti/local-LLM-with-RAG/blob/main/document_loader.py)
📄 [LICENSE](https://github.com/amscotti/local-LLM-with-RAG/blob/main/LICENSE)
📄 [llm.py](https://github.com/amscotti/local-LLM-with-RAG/blob/main/llm.py)
📄 [models.py](https://github.com/amscotti/local-LLM-with-RAG/blob/main/models.py)
📄 [pyproject.toml](https://github.com/amscotti/local-LLM-with-RAG/blob/main/pyproject.toml)
📄 [pyrightconfig.json](https://github.com/amscotti/local-LLM-with-RAG/blob/main/pyrightconfig.json)
📄 [readme.md](https://github.com/amscotti/local-LLM-with-RAG/blob/main/readme.md)
📄 [ui.py](https://github.com/amscotti/local-LLM-with-RAG/blob/main/ui.py)
📄 [uv.lock](https://github.com/amscotti/local-LLM-with-RAG/blob/main/uv.lock)


</details>

## 📞 Call Hierarchy 
<details>
  <summary><strong>Detailed Function Call Hierarchy</strong></summary>

  Here's a structured call hierarchy for the **local-LLM-with-RAG** project, illustrating the main execution flow and important function calls between files:

### Call Hierarchy

```
📁 main() → None [app.py]
├─ 🔷 parse_arguments() → argparse.Namespace [app.py]
│   └─ (parses command line arguments: llm_model_name, embedding_model_name, documents_path)
├─ 🔷 check_if_model_is_available(model_name: str) → None [models.py]
│   ├─ 🔶 __is_model_available_locally(model_name: str) → bool [models.py]
│   └─ 🔶 __pull_model(name: str) → None [models.py]
├─ 🔷 load_documents_into_database(model_name: str, documents_path: str, reload: bool) → Chroma [document_loader.py]
│   ├─ 🔶 load_documents(path: str) → List[Document] [document_loader.py]
│   └─ 🔶 RecursiveCharacterTextSplitter() → (splits documents into chunks)
├─ 🔷 getChatChain(llm, db) → chat function [llm.py]
│   └─ 🔶 chat(question: str) → None [llm.py]
│       └─ 🔷 _combine_documents(docs: list, document_prompt: PromptTemplate, document_separator: str) → str [llm.py]
└─ 🔷 main() → None [ui.py]
    ├─ 🔶 load_documents_into_database() → Chroma [document_loader.py]
    ├─ 🔶 getStreamingChain(question: str, memory, llm, db) → streaming chain [llm.py]
    └─ 🔶 get_list_of_models() → list[str] [models.py]
```

### Explanation of the Call Hierarchy

1. **Entry Point File**: 
   - The entry point for the application is `app.py`, where the `main()` function is defined.

2. **Main Execution Flow**:
   - The `main()` function in `app.py` is responsible for orchestrating the execution of the RAG system. It starts by parsing command-line arguments using `parse_arguments()`.
   - It then checks the availability of the specified model using `check_if_model_is_available()`, which internally calls `__is_model_available_locally()` and `__pull_model()`.
   - After confirming model availability, it loads documents into the Chroma database using `load_documents_into_database()`, which calls `load_documents()` to read and process the documents.
   - Finally, it sets up the chat functionality using `getChatChain()` from `llm.py`, which includes the `chat()` function for handling user queries.

3. **Important Function Calls Between Files**:
   - `app.py` calls functions from `models.py`, `document_loader.py`, and `llm.py` to perform its tasks.
   - The UI file `ui.py` also has a `main()` function that runs the Streamlit application, allowing users to interact with the system. It calls `load_documents_into_database()`, `getStreamingChain()`, and `get_list_of_models()`.

4. **Dependencies Between Modules**:
   - **app.py** depends on:
     - `models.py` for model availability checks.
     - `document_loader.py` for loading documents.
     - `llm.py` for generating chat responses.
   - **document_loader.py** depends on:
     - Various classes and functions from the `langchain` library for document processing.
   - **llm.py** depends on:
     - Functions from the `langchain` library for LLM interactions.
   - **ui.py** depends on:
     - `document_loader.py`, `models.py`, and `llm.py` for managing documents and LLM interactions.

This hierarchy provides a clear overview of how the application flows from the entry point through various files and functions, highlighting the relationships and dependencies between different components of the project.

</details>

## 📈 File Analyses  

<details>
  <summary><strong>File: <a href="https://github.com/amscotti/local-LLM-with-RAG/blob/main/app.py">app.py</a></strong></summary>

  Give a one or two liner description of the code file.  
This code file (`app.py`) serves as the main entry point for a local language model application that utilizes document retrieval and question-answering capabilities.

**1. Main purpose and responsibilities**:  
The main purpose of this file is to initialize and run a local language model (LLM) application that loads documents for retrieval-augmented generation (RAG) and allows users to interact with the model through a command-line interface.

**2. Key functions and their purposes**:  
- `main(llm_model_name: str, embedding_model_name: str, documents_path: str) -> None`: This function expects three string inputs (the names of the LLM model, embedding model, and the path to documents), checks the availability of the specified models, loads documents into a database, initializes the chat interface, and enters a loop to handle user queries until the user decides to exit. It returns nothing.
- `parse_arguments() -> argparse.Namespace`: This function expects no inputs and processes command-line arguments to retrieve the LLM model name, embedding model name, and document path, returning an `argparse.Namespace` object containing these values.

**3. Important interactions with other parts of the system**:  
The `main` function interacts with the `check_if_model_is_available` function from `models` to verify model availability, and it uses `load_documents_into_database` from `document_loader` to load documents into a database. It also utilizes `getChatChain` from `llm` to create a chat interface with the LLM.

**4. Notable features or patterns**:  
The code employs a command-line interface for user interaction, includes error handling for model availability and file loading, and uses a continuous loop to process user input until an exit command is received. It also utilizes the `argparse` library for flexible command-line argument parsing.

Overall, the `app.py` file effectively orchestrates the setup and execution of a local LLM application, enabling users to query a language model based on documents loaded from a specified directory.

  ---
</details>

<details>
  <summary><strong>File: <a href="https://github.com/amscotti/local-LLM-with-RAG/blob/main/document_loader.py">document_loader.py</a></strong></summary>

  **1. Main purpose and responsibilities**:  
The `document_loader.py` file is responsible for loading various document types from a specified directory into a Chroma database, processing the text by splitting it into manageable chunks.

**2. Key functions and their purposes**:  
- **load_documents_into_database(model_name: str, documents_path: str, reload: bool = True) -> Chroma**:  
  This function expects a model name (string), a documents path (string), and a reload flag (boolean). It loads documents from the specified path, splits them into chunks, and then creates embeddings to store them in a Chroma database, returning the Chroma instance.

- **load_documents(path: str) -> List[Document]**:  
  This function expects a path (string) to a directory. It checks if the path exists, raises a FileNotFoundError if it does not, and then loads PDF and Markdown documents from the directory using appropriate loaders, returning a list of Document objects.

**3. Important interactions with other parts of the system**:  
The `document_loader.py` interacts with the `Chroma` vector store for storing document embeddings, `OllamaEmbeddings` for generating embeddings, and utilizes `langchain_community.document_loaders` for loading different document types. It also relies on `RecursiveCharacterTextSplitter` to process the text into chunks.

**4. Notable features or patterns**:  
The code employs a modular approach by separating the loading of documents and the embedding process. It uses dictionary mapping for different file types to streamline the loading process and supports multithreading for efficiency. The use of a persistent storage directory for embeddings is also a notable feature.

Overall, the `document_loader.py` file serves as a crucial component for document ingestion and preprocessing, enabling the system to handle various document formats and prepare them for further analysis or retrieval.

  ---
</details>

<details>
  <summary><strong>File: <a href="https://github.com/amscotti/local-LLM-with-RAG/blob/main/llm.py">llm.py</a></strong></summary>

  Give a one or two liner description of the code file.  
**1. Main purpose and responsibilities**: This file defines functions to interact with a language model (LLM) for answering questions based on provided research documents, utilizing memory for context management and document retrieval.  
**2. Key functions and their purposes**:  
- `getStreamingChain(question: str, memory, llm, db)`: This function expects a question (string), memory (object), llm (language model), and db (database). It processes the question by condensing it, retrieving relevant documents, and generating an answer, returning a streaming response.  
- `getChatChain(llm, db)`: This function expects llm (language model) and db (database). It constructs a chat interaction chain that retrieves documents and generates answers based on user questions, returning a chat function that saves context.  
**3. Important interactions with other parts of the system**: The code interacts with a database to retrieve documents, uses a language model for generating answers, and employs a memory management system to maintain conversation history. It also utilizes prompt templates for formatting inputs to the LLM.  
**4. Notable features or patterns**: The code employs functional programming patterns with the use of lambda functions, integrates memory management for contextual awareness, and utilizes a modular approach to build chains for processing questions and generating answers.  
Overall, this file plays a crucial role in enabling a conversational interface that leverages research documents to provide informed responses to user inquiries.

  ---
</details>

<details>
  <summary><strong>File: <a href="https://github.com/amscotti/local-LLM-with-RAG/blob/main/models.py">models.py</a></strong></summary>

  Give a one or two liner description of the code file.  
**models.py** is responsible for managing and retrieving machine learning models from the Ollama repository, including checking local availability and pulling models as needed.

**1. Main purpose and responsibilities**:  
The main purpose of this file is to interact with the Ollama repository to ensure that specified machine learning models are available locally, and to pull them if they are not.

**2. Key functions and their purposes**:  
- `__pull_model(name: str) -> None`: This function expects a model name as input (string), processes the progress of pulling the model from the Ollama repository, and updates the progress bar accordingly. It does not return any data.
- `__is_model_available_locally(model_name: str) -> bool`: This function expects a model name as input (string), checks if the model is available locally, and returns a boolean indicating its availability.
- `get_list_of_models() -> list[str]`: This function retrieves a list of available models from the Ollama repository and returns it as a list of strings.
- `check_if_model_is_available(model_name: str) -> None`: This function expects a model name as input (string), checks if the model is available locally, and pulls it from the repository if it is not. It raises an exception if there are issues during the process.

**3. Important interactions with other parts of the system**:  
The file interacts with the `ollama` library to perform model management tasks, such as checking model availability and pulling models. It also utilizes the `tqdm` library for displaying progress bars during model downloads.

**4. Notable features or patterns**:  
The code employs private functions (indicated by the double underscore prefix) to encapsulate functionality, ensuring that model pulling and availability checks are modular. It also uses exception handling to manage errors related to model availability and pulling processes.

Overall, **models.py** serves as a crucial component for managing machine learning models, ensuring that the necessary models are readily available for use in the application.

  ---
</details>

<details>
  <summary><strong>File: <a href="https://github.com/amscotti/local-LLM-with-RAG/blob/main/pyproject.toml">pyproject.toml</a></strong></summary>

  Give a one or two liner description of the code file.
**1. Main purpose and responsibilities**: This `pyproject.toml` file defines the metadata, dependencies, and Python version requirements for the project "local-llm-with-rag".

**2. Key functions and their purposes**: 
- The `name` field specifies the project name as a string, which is "local-llm-with-rag".
- The `version` field indicates the current version of the project as a string, set to "0.1.0".
- The `description` field provides a brief description of the project, currently a placeholder.
- The `readme` field points to the README file for the project, which is "README.md".
- The `requires-python` field specifies the minimum Python version required to run the project, which is ">=3.12".
- The `dependencies` list includes specific package versions and requirements necessary for the project to function correctly.

**3. Important interactions with other parts of the system**: This file interacts with the package management system (like Poetry or pip) to install the specified dependencies, ensuring that the project has the necessary libraries to run the code in other files such as `app.py`, `llm.py`, and `ui.py`.

**4. Notable features or patterns**: The file follows the standard structure for a `pyproject.toml`, including sections for project metadata and dependencies, which is common in Python projects to facilitate dependency management and project configuration.

Overall, this `pyproject.toml` file is essential for managing the project's dependencies and ensuring compatibility with the required Python version, thereby enabling a smooth development and deployment process.

  ---
</details>

<details>
  <summary><strong>File: <a href="https://github.com/amscotti/local-LLM-with-RAG/blob/main/ui.py">ui.py</a></strong></summary>

  Give a one or two liner description of the code file.
**1. Main purpose and responsibilities**: The `ui.py` file serves as the user interface for a local LLM (Language Model) application, allowing users to select models, input document paths, and interact with a chat interface for querying indexed documents.

**2. Key functions and their purposes**: 
- `get_list_of_models()`: This function retrieves a list of available models (expected output: list of strings).
- `load_documents_into_database(EMBEDDING_MODEL, folder_path)`: This function takes a string `EMBEDDING_MODEL` and a string `folder_path`, processes the documents in the specified folder to create embeddings, and returns a database object (expected output: database object).
- `getStreamingChain(prompt, messages, llm, db)`: This function expects a string `prompt`, a list of `messages`, an LLM object `llm`, and a database object `db`, processes the input to generate a streaming response, and returns a stream object (expected output: stream object).

**3. Important interactions with other parts of the system**: The file interacts with the `document_loader` to load documents into a database, the `models` module to fetch available models, and the `llm` module to generate responses based on user queries. It also utilizes Streamlit for the web interface and session state management.

**4. Notable features or patterns**: The code employs Streamlit's session state to maintain the state of the selected model, document database, and chat history across user interactions. It also includes error handling for invalid folder paths and provides user feedback through various Streamlit messages.

Overall, `ui.py` effectively integrates user input and document processing to facilitate an interactive querying experience with a local language model, leveraging Streamlit for real-time user interaction and feedback.

  ---
</details>


## ✒️ Project Summary 
This project is a local implementation of a Retrieval-Augmented Generation (RAG) system utilizing Large Language Models (LLMs) to enhance information retrieval and response generation.

1. **Main purpose and functionality**: The primary goal is to run local LLMs for RAG, allowing users to load documents, generate embeddings, and interact with the system to retrieve relevant information through a user-friendly interface.

2. **Tech stack and architecture**: The project is built using Python and leverages several libraries including Ollama for LLMs, Langchain for LLM interactions, Chroma for managing embeddings, and Streamlit for the web interface. The architecture supports both command-line and web-based interactions.

3. **Key components and their interactions**: The core components include `app.py` for application execution, `document_loader.py` for processing documents, `llm.py` for LLM interactions, and `ui.py` for the Streamlit interface. These components work together to load documents, check model availability, and facilitate user queries.

4. **Notable features**: The project allows for the loading of various document types (e.g., PDFs), generates embeddings for efficient retrieval, and provides a chat-like interface for user interaction. It also supports experimentation with different models and configurations.

5. **Code organization and structure**: The repository is organized into directories for core scripts, user interface code, and resources. Key files include `README.md` for documentation, `pyproject.toml` for dependency management, and various Python scripts that encapsulate specific functionalities.

Overall, this repository serves as a comprehensive platform for exploring the integration of local LLMs with retrieval-augmented generation techniques.