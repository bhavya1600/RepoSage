{
  "repository": {
    "name": "ollama_pdf_rag",
    "description": "A demo Jupyter Notebook showcasing a simple local RAG (Retrieval Augmented Generation) pipeline to chat with your PDFs.",
    "language": "Jupyter Notebook"
  },
  "fileMetadata": [
    {
      "path": "run.py",
      "metadata": "{\n  \"name\": \"run.py\",\n  \"path\": \"run.py\",\n  \"imports\": [\n    \"subprocess\",\n    \"sys\",\n    \"Path\"\n  ],\n  \"mainPurpose\": \"Run the Streamlit application.\",\n  \"type\": \"script\",\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"purpose\": \"Run the Streamlit application.\",\n      \"input\": \"\",\n      \"output\": \"\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"streamlit\"\n  ],\n  \"finalReturnType(s)\": \"\"\n}"
    },
    {
      "path": "src/app/main.py",
      "metadata": "{\n  \"name\": \"main.py\",\n  \"path\": \"src/app/main.py\",\n  \"imports\": [\n    \"streamlit as st\",\n    \"logging\",\n    \"os\",\n    \"tempfile\",\n    \"shutil\",\n    \"pdfplumber\",\n    \"ollama\",\n    \"warnings\",\n    \"from langchain_community.document_loaders import UnstructuredPDFLoader\",\n    \"from langchain_ollama import OllamaEmbeddings\",\n    \"from langchain_text_splitters import RecursiveCharacterTextSplitter\",\n    \"from langchain_community.vectorstores import Chroma\",\n    \"from langchain.prompts import ChatPromptTemplate, PromptTemplate\",\n    \"from langchain_core.output_parsers import StrOutputParser\",\n    \"from langchain_ollama import ChatOllama\",\n    \"from langchain_core.runnables import RunnablePassthrough\",\n    \"from langchain.retrievers.multi_query import MultiQueryRetriever\",\n    \"from typing import List, Tuple, Dict, Any, Optional\"\n  ],\n  \"mainPurpose\": \"Streamlit application for PDF-based Retrieval-Augmented Generation (RAG) using Ollama + LangChain.\",\n  \"type\": \"Streamlit Application\",\n  \"functions\": [\n    {\n      \"name\": \"extract_model_names\",\n      \"purpose\": \"Extract model names from the provided models information.\",\n      \"input\": \"models_info: Any\",\n      \"output\": \"Tuple[str, ...]\"\n    },\n    {\n      \"name\": \"create_vector_db\",\n      \"purpose\": \"Create a vector database from an uploaded PDF file.\",\n      \"input\": \"file_upload (st.UploadedFile)\",\n      \"output\": \"Chroma\"\n    },\n    {\n      \"name\": \"process_question\",\n      \"purpose\": \"Process a user question using the vector database and selected language model.\",\n      \"input\": \"question (str), vector_db (Chroma), selected_model (str)\",\n      \"output\": \"str\"\n    },\n    {\n      \"name\": \"extract_all_pages_as_images\",\n      \"purpose\": \"Extract all pages from a PDF file as images.\",\n      \"input\": \"file_upload (st.UploadedFile)\",\n      \"output\": \"List[Any]\"\n    },\n    {\n      \"name\": \"delete_vector_db\",\n      \"purpose\": \"Delete the vector database and clear related session state.\",\n      \"input\": \"vector_db (Optional[Chroma])\",\n      \"output\": \"None\"\n    },\n    {\n      \"name\": \"main\",\n      \"purpose\": \"Main function to run the Streamlit application.\",\n      \"input\": \"\",\n      \"output\": \"None\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"streamlit\",\n    \"logging\",\n    \"os\",\n    \"tempfile\",\n    \"shutil\",\n    \"pdfplumber\",\n    \"ollama\",\n    \"langchain_community\",\n    \"langchain_ollama\",\n    \"langchain_text_splitters\",\n    \"langchain_core\"\n  ],\n  \"finalReturnType(s)\": \"None\"\n}"
    },
    {
      "path": "src/core/document.py",
      "metadata": "{\n  \"name\": \"document.py\",\n  \"path\": \"src/core/document.py\",\n  \"imports\": [\n    \"logging\",\n    \"Path from pathlib\",\n    \"List from typing\",\n    \"UnstructuredPDFLoader from langchain_community.document_loaders\",\n    \"RecursiveCharacterTextSplitter from langchain_text_splitters\"\n  ],\n  \"mainPurpose\": \"Handles PDF document loading and processing.\",\n  \"type\": \"Python module\",\n  \"functions\": [\n    {\n      \"name\": \"__init__\",\n      \"purpose\": \"Initializes the DocumentProcessor with chunk size and overlap.\",\n      \"input\": \"chunk_size: int, chunk_overlap: int\",\n      \"output\": \"None\"\n    },\n    {\n      \"name\": \"load_pdf\",\n      \"purpose\": \"Loads a PDF document from the specified file path.\",\n      \"input\": \"file_path: Path\",\n      \"output\": \"List\"\n    },\n    {\n      \"name\": \"split_documents\",\n      \"purpose\": \"Splits a list of documents into smaller chunks.\",\n      \"input\": \"documents: List\",\n      \"output\": \"List\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"langchain_community.document_loaders\",\n    \"langchain_text_splitters\"\n  ],\n  \"finalReturnType(s)\": \"List\"\n}"
    },
    {
      "path": "src/core/embeddings.py",
      "metadata": "{\n  \"name\": \"embeddings.py\",\n  \"path\": \"src/core/embeddings.py\",\n  \"imports\": [\n    \"logging\",\n    \"List\",\n    \"Path\",\n    \"OllamaEmbeddings\",\n    \"Chroma\"\n  ],\n  \"mainPurpose\": \"Manage vector embeddings and database operations.\",\n  \"type\": \"module\",\n  \"functions\": [\n    {\n      \"name\": \"__init__\",\n      \"purpose\": \"Initialize the VectorStore with a specified embedding model.\",\n      \"input\": \"embedding_model: str\",\n      \"output\": \"None\"\n    },\n    {\n      \"name\": \"create_vector_db\",\n      \"purpose\": \"Create a vector database from a list of documents.\",\n      \"input\": \"documents: List, collection_name: str\",\n      \"output\": \"Chroma\"\n    },\n    {\n      \"name\": \"delete_collection\",\n      \"purpose\": \"Delete the vector database collection if it exists.\",\n      \"input\": \"None\",\n      \"output\": \"None\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"langchain_ollama\",\n    \"langchain_community.vectorstores\"\n  ],\n  \"finalReturnType(s)\": \"Chroma, None\"\n}"
    },
    {
      "path": "src/core/llm.py",
      "metadata": "{\n  \"name\": \"llm.py\",\n  \"path\": \"src/core/llm.py\",\n  \"imports\": [\n    \"logging\",\n    \"from langchain_ollama.chat_models import ChatOllama\",\n    \"from langchain.prompts import ChatPromptTemplate, PromptTemplate\"\n  ],\n  \"mainPurpose\": \"Manages LLM configuration and prompts.\",\n  \"type\": \"Python module\",\n  \"functions\": [\n    {\n      \"name\": \"__init__\",\n      \"purpose\": \"Initializes the LLMManager with a specified model name.\",\n      \"input\": \"model_name: str (default 'llama2')\",\n      \"output\": \"None\"\n    },\n    {\n      \"name\": \"get_query_prompt\",\n      \"purpose\": \"Generates a prompt template for query generation.\",\n      \"input\": \"None\",\n      \"output\": \"PromptTemplate\"\n    },\n    {\n      \"name\": \"get_rag_prompt\",\n      \"purpose\": \"Generates a prompt template for RAG (Retrieval-Augmented Generation).\",\n      \"input\": \"None\",\n      \"output\": \"ChatPromptTemplate\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"langchain_ollama\",\n    \"langchain\"\n  ],\n  \"finalReturnType(s)\": \"PromptTemplate, ChatPromptTemplate\"\n}"
    },
    {
      "path": "src/core/rag.py",
      "metadata": "{\n  \"name\": \"rag.py\",\n  \"path\": \"src/core/rag.py\",\n  \"imports\": [\n    \"logging\",\n    \"Any\",\n    \"Dict\",\n    \"RunnablePassthrough\",\n    \"StrOutputParser\",\n    \"MultiQueryRetriever\",\n    \"LLMManager\"\n  ],\n  \"mainPurpose\": \"Manages the RAG (Retrieval Augmented Generation) pipeline.\",\n  \"type\": \"class\",\n  \"functions\": [\n    {\n      \"name\": \"__init__\",\n      \"purpose\": \"Initializes the RAGPipeline with a vector database and LLM manager.\",\n      \"input\": \"vector_db: Any, llm_manager: LLMManager\",\n      \"output\": \"None\"\n    },\n    {\n      \"name\": \"_setup_retriever\",\n      \"purpose\": \"Sets up the multi-query retriever.\",\n      \"input\": \"None\",\n      \"output\": \"MultiQueryRetriever\"\n    },\n    {\n      \"name\": \"_setup_chain\",\n      \"purpose\": \"Sets up the RAG chain.\",\n      \"input\": \"None\",\n      \"output\": \"Any\"\n    },\n    {\n      \"name\": \"get_response\",\n      \"purpose\": \"Gets response for a question using the RAG pipeline.\",\n      \"input\": \"question: str\",\n      \"output\": \"str\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"langchain_core.runnables\",\n    \"langchain_core.output_parsers\",\n    \"langchain.retrievers.multi_query\"\n  ],\n  \"finalReturnType(s)\": \"str\"\n}"
    },
    {
      "path": "tests/test_document.py",
      "metadata": "{\n  \"name\": \"test_document.py\",\n  \"path\": \"tests/test_document.py\",\n  \"imports\": [\n    \"pytest\",\n    \"Path from pathlib\",\n    \"Mock and patch from unittest.mock\",\n    \"DocumentProcessor from src.core.document\",\n    \"Document from langchain_core.documents\"\n  ],\n  \"mainPurpose\": \"Test document processing functionality.\",\n  \"type\": \"test\",\n  \"functions\": [\n    {\n      \"name\": \"processor\",\n      \"purpose\": \"Create a DocumentProcessor instance.\",\n      \"input\": \"\",\n      \"output\": \"DocumentProcessor instance\"\n    },\n    {\n      \"name\": \"test_pdf_path\",\n      \"purpose\": \"Get the test PDF path.\",\n      \"input\": \"\",\n      \"output\": \"Path to the test PDF file\"\n    },\n    {\n      \"name\": \"test_init\",\n      \"purpose\": \"Test initialization.\",\n      \"input\": \"processor\",\n      \"output\": \"Assertions on chunk_size and chunk_overlap\"\n    },\n    {\n      \"name\": \"test_init_custom_params\",\n      \"purpose\": \"Test initialization with custom parameters.\",\n      \"input\": \"\",\n      \"output\": \"Assertions on custom chunk_size and chunk_overlap\"\n    },\n    {\n      \"name\": \"test_load_pdf_file_not_found\",\n      \"purpose\": \"Test loading non-existent PDF.\",\n      \"input\": \"mock_load\",\n      \"output\": \"Raises FileNotFoundError\"\n    },\n    {\n      \"name\": \"test_load_pdf_success\",\n      \"purpose\": \"Test loading existing PDF.\",\n      \"input\": \"processor, test_pdf_path\",\n      \"output\": \"Assertions on loaded documents\"\n    },\n    {\n      \"name\": \"test_split_documents\",\n      \"purpose\": \"Test document splitting.\",\n      \"input\": \"processor\",\n      \"output\": \"Assertions on number of chunks\"\n    },\n    {\n      \"name\": \"test_split_empty_document\",\n      \"purpose\": \"Test splitting empty document.\",\n      \"input\": \"processor\",\n      \"output\": \"Assertions on chunks from empty document\"\n    },\n    {\n      \"name\": \"test_split_large_document\",\n      \"purpose\": \"Test splitting very large document.\",\n      \"input\": \"processor\",\n      \"output\": \"Assertions on chunk sizes\"\n    },\n    {\n      \"name\": \"test_metadata_preservation\",\n      \"purpose\": \"Test metadata is preserved during splitting.\",\n      \"input\": \"processor\",\n      \"output\": \"Assertions on metadata in chunks\"\n    },\n    {\n      \"name\": \"test_chunk_overlap\",\n      \"purpose\": \"Test chunk overlap is working correctly.\",\n      \"input\": \"\",\n      \"output\": \"Assertions on overlap between chunks\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"pytest\",\n    \"unittest.mock\",\n    \"src.core.document\",\n    \"langchain_core.documents\"\n  ],\n  \"finalReturnType(s)\": \"\"\n}"
    },
    {
      "path": "tests/test_models.py",
      "metadata": "{\n  \"name\": \"test_models.py\",\n  \"path\": \"tests/test_models.py\",\n  \"imports\": [\n    \"pytest\",\n    \"Mock from unittest.mock\",\n    \"extract_model_names from src.app.main\"\n  ],\n  \"mainPurpose\": \"Test model extraction functionality.\",\n  \"type\": \"test\",\n  \"functions\": [\n    {\n      \"name\": \"test_extract_model_names_empty\",\n      \"purpose\": \"Test extracting model names from empty response.\",\n      \"input\": \"models_info with empty models list\",\n      \"output\": \"tuple()\"\n    },\n    {\n      \"name\": \"test_extract_model_names_success\",\n      \"purpose\": \"Test successful model name extraction.\",\n      \"input\": \"models_info with two mock Model objects\",\n      \"output\": \"(\\\"model1:latest\\\", \\\"model2:latest\\\")\"\n    },\n    {\n      \"name\": \"test_extract_model_names_invalid_format\",\n      \"purpose\": \"Test handling invalid response format.\",\n      \"input\": \"models_info with invalid format\",\n      \"output\": \"tuple()\"\n    },\n    {\n      \"name\": \"test_extract_model_names_exception\",\n      \"purpose\": \"Test handling exceptions during extraction.\",\n      \"input\": \"models_info without models attribute\",\n      \"output\": \"tuple()\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"pytest\",\n    \"unittest.mock\"\n  ],\n  \"finalReturnType(s)\": \"tuple\"\n}"
    },
    {
      "path": "tests/test_rag.py",
      "metadata": "{\n  \"name\": \"test_rag.py\",\n  \"path\": \"tests/test_rag.py\",\n  \"imports\": [\n    \"unittest\",\n    \"unittest.mock.Mock\",\n    \"unittest.mock.patch\",\n    \"unittest.mock.MagicMock\",\n    \"src.core.rag.RAGPipeline\",\n    \"langchain_core.documents.Document\",\n    \"langchain.retrievers.multi_query.MultiQueryRetriever\",\n    \"langchain_core.output_parsers.StrOutputParser\"\n  ],\n  \"mainPurpose\": \"Test RAG functionality.\",\n  \"type\": \"unit test\",\n  \"functions\": [\n    {\n      \"name\": \"setUp\",\n      \"purpose\": \"Set up test cases.\",\n      \"input\": \"\",\n      \"output\": \"\"\n    },\n    {\n      \"name\": \"tearDown\",\n      \"purpose\": \"Clean up patches.\",\n      \"input\": \"\",\n      \"output\": \"\"\n    },\n    {\n      \"name\": \"test_setup_retriever\",\n      \"purpose\": \"Test retriever setup.\",\n      \"input\": \"\",\n      \"output\": \"\"\n    },\n    {\n      \"name\": \"test_setup_chain\",\n      \"purpose\": \"Test chain setup.\",\n      \"input\": \"\",\n      \"output\": \"\"\n    },\n    {\n      \"name\": \"test_get_response\",\n      \"purpose\": \"Test getting response from the RAG pipeline.\",\n      \"input\": \"question (str)\",\n      \"output\": \"response (str)\"\n    },\n    {\n      \"name\": \"test_get_response_empty_question\",\n      \"purpose\": \"Test handling empty question.\",\n      \"input\": \"\",\n      \"output\": \"\"\n    },\n    {\n      \"name\": \"test_get_response_long_question\",\n      \"purpose\": \"Test handling very long question.\",\n      \"input\": \"long_question (str)\",\n      \"output\": \"response (str)\"\n    },\n    {\n      \"name\": \"test_get_response_special_characters\",\n      \"purpose\": \"Test handling questions with special characters.\",\n      \"input\": \"special_question (str)\",\n      \"output\": \"response (str)\"\n    },\n    {\n      \"name\": \"test_chain_error_handling\",\n      \"purpose\": \"Test error handling in the chain.\",\n      \"input\": \"question (str)\",\n      \"output\": \"raises Exception\"\n    },\n    {\n      \"name\": \"test_retriever_error_handling\",\n      \"purpose\": \"Test error handling in the retriever setup.\",\n      \"input\": \"\",\n      \"output\": \"raises Exception\"\n    },\n    {\n      \"name\": \"test_memory_cleanup\",\n      \"purpose\": \"Test proper cleanup of resources.\",\n      \"input\": \"\",\n      \"output\": \"\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"unittest\",\n    \"unittest.mock\",\n    \"src.core.rag\",\n    \"langchain_core.documents\",\n    \"langchain.retrievers.multi_query\",\n    \"langchain_core.output_parsers\"\n  ],\n  \"finalReturnType(s)\": \"\"\n}"
    }
  ],
  "callHierarchy": "Based on the provided project understanding and file metadata, here’s a structured call hierarchy for the `ollama_pdf_rag` project. This hierarchy outlines the entry point, main execution flow, important function calls, and dependencies between modules.\n\n### 1. Entry Point File\n- **File**: `run.py`\n  - **Main Function**: `main()`\n    - **Purpose**: Runs the Streamlit application.\n\n### 2. Main Execution Flow\n1. **`run.py`**:\n   - Calls `main()` function.\n   - Initializes and runs the Streamlit application defined in `src/app/main.py`.\n\n2. **`src/app/main.py`**:\n   - **Main Function**: `main()`\n     - Sets up the Streamlit interface.\n     - Handles user interactions (PDF uploads, questions).\n     - Calls various helper functions based on user input:\n       - `create_vector_db(file_upload)`: Creates a vector database from uploaded PDF.\n       - `process_question(question, vector_db, selected_model)`: Processes user questions using the vector database and selected model.\n       - `delete_vector_db(vector_db)`: Deletes the vector database and clears session state.\n\n3. **`src/core/document.py`**:\n   - **Function**: `load_pdf(file_path)`\n     - Loads a PDF document and returns its content.\n   - **Function**: `split_documents(documents)`\n     - Splits the loaded documents into smaller chunks.\n\n4. **`src/core/embeddings.py`**:\n   - **Function**: `create_vector_db(documents, collection_name)`\n     - Creates a vector database from a list of documents.\n\n5. **`src/core/llm.py`**:\n   - **Function**: `get_query_prompt()`\n     - Generates a prompt template for query generation.\n   - **Function**: `get_rag_prompt()`\n     - Generates a prompt template for RAG.\n\n6. **`src/core/rag.py`**:\n   - **Function**: `get_response(question)`\n     - Uses the RAG pipeline to get a response for the user's question.\n\n### 3. Important Function Calls Between Files\n- **From `run.py` to `src/app/main.py`**:\n  - `main()` → `src/app/main.py:main()`\n\n- **From `src/app/main.py` to `src/core/document.py`**:\n  - `create_vector_db(file_upload)` → `src/core/document.py:load_pdf(file_path)`\n  - `create_vector_db(file_upload)` → `src/core/document.py:split_documents(documents)`\n\n- **From `src/app/main.py` to `src/core/embeddings.py`**:\n  - `create_vector_db(file_upload)` → `src/core/embeddings.py:create_vector_db(documents, collection_name)`\n\n- **From `src/app/main.py` to `src/core/llm.py`**:\n  - `process_question(question, vector_db, selected_model)` → `src/core/llm.py:get_query_prompt()`\n  - `process_question(question, vector_db, selected_model)` → `src/core/llm.py:get_rag_prompt()`\n\n- **From `src/app/main.py` to `src/core/rag.py`**:\n  - `process_question(question, vector_db, selected_model)` → `src/core/rag.py:get_response(question)`\n\n### 4. Dependencies Between Modules\n- **`run.py`**: Depends on `streamlit`.\n- **`src/app/main.py`**: Depends on:\n  - `streamlit`\n  - `pdfplumber`\n  - `ollama`\n  - `langchain_community`\n  - `langchain_ollama`\n  - `langchain_text_splitters`\n  - `langchain_core`\n- **`src/core/document.py`**: Depends on:\n  - `langchain_community.document_loaders`\n  - `langchain_text_splitters`\n- **`src/core/embeddings.py`**: Depends on:\n  - `langchain_ollama`\n  - `langchain_community.vectorstores`\n- **`src/core/llm.py`**: Depends on:\n  - `langchain_ollama`\n  - `langchain`\n- **`src/core/rag.py`**: Depends on:\n  - `langchain_core.runnables`\n  - `langchain_core.output_parsers`\n  - `langchain.retrievers.multi_query`\n\n### 5. Visual Mapping of Function Calls\n```plaintext\nrun.py\n  └── main()\n      └── src/app/main.py\n          └── main()\n              ├── create_vector_db(file_upload)\n              │   └── src/core/document.py\n              │       ├── load_pdf(file_path)\n              │       └── split_documents(documents)\n              │           └── src/core/embeddings.py\n              │               └── create_vector_db(documents, collection_name)\n              └── process_question(question, vector_db, selected_model)\n                  ├── src/core/llm.py\n                  │   ├── get_query_prompt()\n                  │   └── get_rag_prompt()\n                  └── src/core/rag.py\n                      └── get_response(question)\n```\n\nThis structured call hierarchy provides a clear view of how the application flows from the entry point through various files and functions, highlighting the important function calls and dependencies between modules."
}