{
  "repository": {
    "name": "local-LLM-with-RAG",
    "description": "Running local Language Language Models (LLM) to perform Retrieval-Augmented Generation (RAG)",
    "language": "Python"
  },
  "fileMetadata": [
    {
      "path": "app.py",
      "metadata": "{\n  \"name\": \"app.py\",\n  \"path\": \"app.py\",\n  \"imports\": [\n    \"from langchain_ollama import ChatOllama\",\n    \"from models import check_if_model_is_available\",\n    \"from document_loader import load_documents_into_database\",\n    \"import argparse\",\n    \"import sys\",\n    \"from llm import getChatChain\"\n  ],\n  \"mainPurpose\": \"To run a local LLM with Retrieval-Augmented Generation (RAG) using Ollama.\",\n  \"type\": \"script\",\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"purpose\": \"Main function to check model availability, load documents, and handle user interaction.\",\n      \"input\": \"llm_model_name (str), embedding_model_name (str), documents_path (str)\",\n      \"output\": \"None\"\n    },\n    {\n      \"name\": \"parse_arguments\",\n      \"purpose\": \"Parse command-line arguments for model and document path.\",\n      \"input\": \"\",\n      \"output\": \"argparse.Namespace\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"langchain_ollama\",\n    \"models\",\n    \"document_loader\",\n    \"argparse\",\n    \"sys\",\n    \"llm\"\n  ],\n  \"finalReturnType(s)\": \"None\"\n}"
    },
    {
      "path": "document_loader.py",
      "metadata": "{\n  \"name\": \"document_loader.py\",\n  \"path\": \"document_loader.py\",\n  \"imports\": [\n    \"from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\",\n    \"import os\",\n    \"from typing import List\",\n    \"from langchain_core.documents import Document\",\n    \"from langchain_ollama import OllamaEmbeddings\",\n    \"from langchain_community.vectorstores import Chroma\",\n    \"from langchain.text_splitter import RecursiveCharacterTextSplitter\"\n  ],\n  \"mainPurpose\": \"To load documents from a specified directory into a Chroma database after splitting the text into chunks.\",\n  \"type\": \"module\",\n  \"functions\": [\n    {\n      \"name\": \"load_documents_into_database\",\n      \"purpose\": \"Loads documents from the specified directory into the Chroma database after splitting the text into chunks.\",\n      \"input\": \"model_name: str, documents_path: str, reload: bool\",\n      \"output\": \"Chroma\"\n    },\n    {\n      \"name\": \"load_documents\",\n      \"purpose\": \"Loads documents from the specified directory path, supporting PDF and Markdown file types.\",\n      \"input\": \"path: str\",\n      \"output\": \"List[Document]\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"langchain_community\",\n    \"os\",\n    \"typing\",\n    \"langchain_core\",\n    \"langchain_ollama\",\n    \"langchain_community.vectorstores\",\n    \"langchain.text_splitter\"\n  ],\n  \"finalReturnType(s)\": \"Chroma, List[Document]\"\n}"
    },
    {
      "path": "llm.py",
      "metadata": "{\n  \"name\": \"llm.py\",\n  \"path\": \"llm.py\",\n  \"imports\": [\n    \"itemgetter from operator\",\n    \"StreamingStdOutCallbackHandler from langchain.callbacks.streaming_stdout\",\n    \"ConversationBufferMemory from langchain.memory\",\n    \"ChatPromptTemplate from langchain_core.prompts\",\n    \"RunnableLambda, RunnablePassthrough from langchain_core.runnables\",\n    \"get_buffer_string from langchain_core.messages\",\n    \"format_document from langchain_core.prompts\",\n    \"PromptTemplate from langchain.prompts.prompt\"\n  ],\n  \"mainPurpose\": \"To define functions for processing questions and retrieving answers using a language model and memory.\",\n  \"type\": \"module\",\n  \"functions\": [\n    {\n      \"name\": \"_combine_documents\",\n      \"purpose\": \"Combines multiple documents into a single string using a specified prompt template.\",\n      \"input\": \"docs (list of documents), document_prompt (PromptTemplate), document_separator (str)\",\n      \"output\": \"str\"\n    },\n    {\n      \"name\": \"getStreamingChain\",\n      \"purpose\": \"Creates a streaming chain for processing a question using memory and a language model.\",\n      \"input\": \"question (str), memory, llm, db\",\n      \"output\": \"streaming chain\"\n    },\n    {\n      \"name\": \"getChatChain\",\n      \"purpose\": \"Creates a chat chain for processing a question and retrieving answers using memory and a language model.\",\n      \"input\": \"llm, db\",\n      \"output\": \"chat function\"\n    },\n    {\n      \"name\": \"chat\",\n      \"purpose\": \"Processes a question and retrieves an answer, saving the context in memory.\",\n      \"input\": \"question (str)\",\n      \"output\": \"None\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"langchain\",\n    \"langchain_core\"\n  ],\n  \"finalReturnType(s)\": \"streaming chain, chat function\"\n}"
    },
    {
      "path": "models.py",
      "metadata": "{\n  \"name\": \"models.py\",\n  \"path\": \"models.py\",\n  \"imports\": [\n    \"ollama\",\n    \"tqdm\"\n  ],\n  \"mainPurpose\": \"Manage and retrieve machine learning models from the Ollama repository.\",\n  \"type\": \"module\",\n  \"functions\": [\n    {\n      \"name\": \"__pull_model\",\n      \"purpose\": \"Pulls a model from the Ollama repository and shows progress.\",\n      \"input\": \"name: str\",\n      \"output\": \"None\"\n    },\n    {\n      \"name\": \"__is_model_available_locally\",\n      \"purpose\": \"Checks if a specified model is available locally.\",\n      \"input\": \"model_name: str\",\n      \"output\": \"bool\"\n    },\n    {\n      \"name\": \"get_list_of_models\",\n      \"purpose\": \"Retrieves a list of available models from the Ollama repository.\",\n      \"input\": \"\",\n      \"output\": \"list[str]\"\n    },\n    {\n      \"name\": \"check_if_model_is_available\",\n      \"purpose\": \"Checks if a model is available locally and pulls it if not.\",\n      \"input\": \"model_name: str\",\n      \"output\": \"None\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"ollama\",\n    \"tqdm\"\n  ],\n  \"finalReturnType(s)\": \"None, bool, list[str]\"\n}"
    },
    {
      "path": "pyproject.toml",
      "metadata": "{\n  \"name\": \"local-llm-with-rag\",\n  \"path\": \"pyproject.toml\",\n  \"imports\": [],\n  \"mainPurpose\": \"Configuration file for a Python project specifying dependencies and project metadata.\",\n  \"type\": \"configuration\",\n  \"functions\": [],\n  \"exports\": [],\n  \"dependencies\": [\n    \"chromadb==0.6.3\",\n    \"langchain==0.3.18\",\n    \"langchain-community==0.3.17\",\n    \"langchain-ollama>=0.2.3\",\n    \"ollama==0.4.7\",\n    \"pypdf==5.3.0\",\n    \"streamlit==1.42.0\",\n    \"tqdm==4.67.1\",\n    \"watchdog==6.0.0\"\n  ],\n  \"finalReturnType(s)\": \"\"\n}"
    },
    {
      "path": "ui.py",
      "metadata": "{\n  \"name\": \"ui.py\",\n  \"path\": \"ui.py\",\n  \"imports\": [\n    \"streamlit as st\",\n    \"os\",\n    \"langchain_ollama.ChatOllama\",\n    \"document_loader.load_documents_into_database\",\n    \"models.get_list_of_models\",\n    \"llm.getStreamingChain\"\n  ],\n  \"mainPurpose\": \"To create a user interface for interacting with a local LLM (Language Model) using a Retrieval-Augmented Generation (RAG) approach.\",\n  \"type\": \"Streamlit application\",\n  \"functions\": [\n    {\n      \"name\": \"get_list_of_models\",\n      \"purpose\": \"Retrieve a list of available models for selection.\",\n      \"input\": \"\",\n      \"output\": \"List of model names.\"\n    },\n    {\n      \"name\": \"load_documents_into_database\",\n      \"purpose\": \"Load documents from a specified folder into a database after creating embeddings.\",\n      \"input\": \"Embedding model and folder path.\",\n      \"output\": \"Database object.\"\n    },\n    {\n      \"name\": \"getStreamingChain\",\n      \"purpose\": \"Generate a response from the LLM based on user input and chat history.\",\n      \"input\": \"User prompt, chat messages, LLM instance, and database.\",\n      \"output\": \"Streaming response from the LLM.\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"streamlit\",\n    \"os\",\n    \"langchain_ollama\",\n    \"document_loader\",\n    \"models\",\n    \"llm\"\n  ],\n  \"finalReturnType(s)\": \"None\"\n}"
    }
  ],
  "callHierarchy": "Here's a structured call hierarchy for the **local-LLM-with-RAG** project, detailing the main execution flow, important function calls between files, and dependencies between modules.\n\n### Call Hierarchy\n\n```\n📁 main() → None [app.py]\n├─ 🔷 parse_arguments() → argparse.Namespace [app.py]\n│   └─ (parses command-line arguments)\n├─ 🔷 check_if_model_is_available(llm_model_name: str) → None [models.py]\n│   └─ 🔶 __is_model_available_locally(model_name: str) → bool [models.py]\n│       └─ (checks local model availability)\n├─ 🔷 load_documents_into_database(model_name: str, documents_path: str, reload: bool) → Chroma [document_loader.py]\n│   ├─ 🔶 load_documents(path: str) → List[Document] [document_loader.py]\n│   │   └─ (loads documents from the specified directory)\n│   └─ (loads documents into Chroma database after splitting text)\n└─ 🔷 getChatChain(llm, db) → chat function [llm.py]\n    └─ 🔶 chat(question: str) → None [llm.py]\n        └─ (processes a question and retrieves an answer)\n```\n\n### Entry Point File\n- **`app.py`**: This is the main entry point of the application where the execution starts.\n\n### Main Execution Flow\n1. **`main()`** is called, which serves as the entry point.\n2. **`parse_arguments()`** is invoked to parse command-line arguments for model and document path.\n3. **`check_if_model_is_available(llm_model_name)`** checks if the specified LLM model is available locally.\n   - Calls **`__is_model_available_locally(model_name)`** to verify local availability.\n4. **`load_documents_into_database(model_name, documents_path, reload)`** is called to load documents into the database.\n   - Calls **`load_documents(path)`** to load documents from the specified directory.\n5. **`getChatChain(llm, db)`** is called to create a chat function for processing questions.\n   - Calls **`chat(question)`** to process user questions and retrieve answers.\n\n### Important Function Calls Between Files\n- **`app.py`**\n  - Calls functions from **`models.py`**, **`document_loader.py`**, and **`llm.py`**.\n- **`document_loader.py`**\n  - Loads documents and interacts with the Chroma database.\n- **`llm.py`**\n  - Handles the logic for querying the LLM and managing chat interactions.\n\n### Dependencies Between Modules\n- **`app.py`** depends on:\n  - `models` (for model availability checks)\n  - `document_loader` (for loading documents)\n  - `llm` (for LLM interaction)\n- **`document_loader.py`** depends on:\n  - `langchain_community.document_loaders` (for loading documents)\n  - `langchain_ollama` (for embeddings)\n  - `langchain_community.vectorstores` (for Chroma database)\n- **`llm.py`** depends on:\n  - `langchain` (for LLM processing)\n- **`models.py`** depends on:\n  - `ollama` (for model management)\n\nThis structured hierarchy provides a clear understanding of how the application flows from the entry point through various files and functions, highlighting the main execution path and important function calls between files."
}