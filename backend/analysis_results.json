{
  "repository": {
    "name": "local-LLM-with-RAG",
    "description": "Running local Language Language Models (LLM) to perform Retrieval-Augmented Generation (RAG)",
    "language": "Python"
  },
  "projectUnderstanding": "1. **Project Overview**: The project aims to run local Large Language Models (LLMs) with Ollama to perform Retrieval-Augmented Generation (RAG) for answering questions based on sample PDFs. It includes a command-line interface and a Streamlit web UI for interaction.\n\n2. **Main Components**:\n   - **app.py**: Main script for running the project, handling model loading, document processing, and RAG.\n   - **document_loader.py**: Handles loading and processing of documents.\n   - **llm.py**: Contains logic for interacting with the LLM.\n   - **models.py**: Manages model configurations and loading.\n   - **ui.py**: Script for running the Streamlit UI.\n   - **Research**: Directory containing sample PDF and Markdown files for testing.\n   - **images**: Directory for storing images used in the README.\n\n3. **Tech Stack**:\n   - **Languages**: Python\n   - **Libraries/Frameworks**:\n     - [Langchain](https://github.com/langchain/langchain): For working with LLMs.\n     - [Ollama](https://ollama.ai/): For running LLMs locally.\n     - [Chroma](https://docs.trychroma.com/): For vector database operations.\n     - [PyPDF2](https://pypi.org/project/PyPDF2/): For PDF file manipulation.\n     - [Streamlit](https://streamlit.io/): For creating the web UI.\n     - [UV](https://astral.sh/uv): For package installation and management.\n\n4. **Architecture**:\n   - **High-Level Pattern**: The project follows a modular architecture with clear separation of concerns. The main components are:\n     - **Data Loading**: `document_loader.py` handles loading and preprocessing of documents.\n     - **Model Management**: `models.py` and `llm.py` manage model configurations and interactions.\n     - **Main Application Logic**: `app.py` orchestrates the entire process, including document processing, embedding generation, and RAG.\n     - **Web Interface**: `ui.py` provides a Streamlit-based UI for user interaction.\n\n5. **Key Limitations/Constraints**:\n   - **Embedding Redownload**: The embeddings are reloaded each time the application runs, which is not efficient and is only done for testing purposes.\n   - **Static Document Directory**: The `Research` directory contains static documents, which may not be ideal for dynamic or large-scale applications.\n   - **Command-Line Dependency**: The main application relies on command-line arguments for specifying models and document paths, which might be cumbersome for frequent use.",
  "fileTree": {
    "name": "root",
    "type": "directory",
    "children": {
      "images": {
        "name": "images",
        "type": "directory",
        "children": {
          "streamlit_ui.png": {
            "name": "streamlit_ui.png",
            "type": "blob",
            "children": {}
          },
          "wizard_experimenting.jpg": {
            "name": "wizard_experimenting.jpg",
            "type": "blob",
            "children": {}
          }
        }
      },
      "Research": {
        "name": "Research",
        "type": "directory",
        "children": {
          "2304.03442v1.pdf": {
            "name": "2304.03442v1.pdf",
            "type": "blob",
            "children": {}
          },
          "2305.14325.pdf": {
            "name": "2305.14325.pdf",
            "type": "blob",
            "children": {}
          },
          "2308.10848.pdf": {
            "name": "2308.10848.pdf",
            "type": "blob",
            "children": {}
          },
          "2309.14391.pdf": {
            "name": "2309.14391.pdf",
            "type": "blob",
            "children": {}
          }
        }
      },
      ".gitignore": {
        "name": ".gitignore",
        "type": "blob",
        "children": {}
      },
      "app.py": {
        "name": "app.py",
        "type": "blob",
        "children": {}
      },
      "document_loader.py": {
        "name": "document_loader.py",
        "type": "blob",
        "children": {}
      },
      "LICENSE": {
        "name": "LICENSE",
        "type": "blob",
        "children": {}
      },
      "llm.py": {
        "name": "llm.py",
        "type": "blob",
        "children": {}
      },
      "models.py": {
        "name": "models.py",
        "type": "blob",
        "children": {}
      },
      "pyproject.toml": {
        "name": "pyproject.toml",
        "type": "blob",
        "children": {}
      },
      "pyrightconfig.json": {
        "name": "pyrightconfig.json",
        "type": "blob",
        "children": {}
      },
      "readme.md": {
        "name": "readme.md",
        "type": "blob",
        "children": {}
      },
      "ui.py": {
        "name": "ui.py",
        "type": "blob",
        "children": {}
      },
      "uv.lock": {
        "name": "uv.lock",
        "type": "blob",
        "children": {}
      }
    }
  },
  "fileMetadata": [
    {
      "path": "app.py",
      "metadata": {
        "name": "app.py",
        "path": "./app.py",
        "imports": [
          "langchain_ollama",
          "models",
          "document_loader",
          "argparse",
          "sys",
          "llm"
        ],
        "mainPurpose": "Runs a local Large Language Model (LLM) with Retrieval-Augmented Generation (RAG) using Ollama.",
        "type": "Python script",
        "functions": [
          {
            "name": "main",
            "purpose": "Executes the main logic of the application, including loading models, checking their availability, loading documents into a database, and initiating the chat interface.",
            "input": "llm_model_name: str, embedding_model_name: str, documents_path: str",
            "output": "None"
          },
          {
            "name": "parse_arguments",
            "purpose": "Parses command-line arguments for the script.",
            "input": "()",
            "output": "argparse.Namespace"
          }
        ],
        "exports": [],
        "dependencies": [
          "langchain_ollama",
          "models",
          "document_loader"
        ],
        "finalReturnType(s)": "None"
      }
    },
    {
      "path": "document_loader.py",
      "metadata": {
        "name": "document_loader.py",
        "path": "document_loader.py",
        "imports": [
          "langchain_community.document_loaders",
          "os",
          "typing",
          "langchain_core.documents",
          "langchain_ollama",
          "langchain_community.vectorstores",
          "langchain.text_splitter"
        ],
        "mainPurpose": "To load documents from specified directories into a Chroma database",
        "type": "Python script",
        "functions": [
          {
            "name": "load_documents_into_database",
            "purpose": "Loads documents from the specified directory into the Chroma database after splitting the text into chunks",
            "input": "model_name: str, documents_path: str, reload: bool = True",
            "output": "Chroma"
          },
          {
            "name": "load_documents",
            "purpose": "Loads documents from the specified directory path",
            "input": "path: str",
            "output": "List[Document]"
          }
        ],
        "exports": [],
        "dependencies": [
          "langchain_community",
          "langchain_core",
          "langchain_ollama",
          "langchain"
        ],
        "finalReturnType(s)": "Chroma, List[Document]"
      }
    },
    {
      "path": "llm.py",
      "metadata": {
        "name": "llm.py",
        "path": "llm.py",
        "imports": [
          "itemgetter",
          "StreamingStdOutCallbackHandler",
          "ConversationBufferMemory",
          "ChatPromptTemplate",
          "RunnableLambda",
          "RunnablePassthrough",
          "get_buffer_string",
          "format_document",
          "PromptTemplate"
        ],
        "mainPurpose": "Define functions for generating and executing chains to generate responses based on questions and research documents using LangChain library.",
        "type": "Python",
        "functions": [
          {
            "name": "_combine_documents",
            "purpose": "Combine multiple documents into a single string with a specified separator.",
            "input": "docs (list of documents), document_prompt (optional), document_separator (optional)",
            "output": "str"
          },
          {
            "name": "getStreamingChain",
            "purpose": "Create a streaming chain to generate a response based on a question and memory using a language model.",
            "input": "question (str), memory (ConversationBufferMemory), llm (language model), db (database)",
            "output": "RunnableLambda"
          },
          {
            "name": "getChatChain",
            "purpose": "Create a chat chain to generate a response based on a question and memory using a language model.",
            "input": "llm (language model), db (database)",
            "output": "function"
          }
        ],
        "exports": [
          "getStreamingChain",
          "getChatChain"
        ],
        "dependencies": [
          "langchain",
          "langchain_core"
        ],
        "finalReturnType(s)": "RunnableLambda, function"
      }
    },
    {
      "path": "models.py",
      "metadata": {
        "name": "models.py",
        "path": "models.py",
        "imports": [
          "ollama",
          "tqdm"
        ],
        "mainPurpose": "Manages model availability and retrieval from the Ollama repository.",
        "type": "Python module",
        "functions": [
          {
            "name": "__pull_model",
            "purpose": "Pulls a model from the Ollama repository and updates the progress bar.",
            "input": "name: str",
            "output": "None"
          },
          {
            "name": "__is_model_available_locally",
            "purpose": "Checks if a model is available locally.",
            "input": "model_name: str",
            "output": "bool"
          },
          {
            "name": "get_list_of_models",
            "purpose": "Retrieves a list of available models from the Ollama repository.",
            "input": "",
            "output": "list[str]"
          },
          {
            "name": "check_if_model_is_available",
            "purpose": "Ensures that the specified model is available locally, pulling it if necessary.",
            "input": "model_name: str",
            "output": "None"
          }
        ],
        "exports": [],
        "dependencies": [
          "ollama",
          "tqdm"
        ],
        "finalReturnType(s)": "None, list[str]"
      }
    },
    {
      "path": "ui.py",
      "metadata": {
        "name": "ui.py",
        "path": "ui.py",
        "imports": [
          "streamlit as st",
          "os",
          "langchain_ollama.ChatOllama",
          "document_loader.load_documents_into_database",
          "models.get_list_of_models",
          "llm.getStreamingChain"
        ],
        "mainPurpose": "To create a Streamlit UI for interacting with a local Large Language Model using Reinforced AI.",
        "type": "Python script",
        "functions": [
          {
            "name": "getStreamingChain",
            "purpose": "To get a streaming chain for generating responses from the LLM.",
            "input": "prompt, messages, llm, db",
            "output": "stream"
          }
        ],
        "exports": [],
        "dependencies": [
          "streamlit",
          "langchain_ollama",
          "document_loader",
          "models",
          "llm"
        ],
        "finalReturnType(s)": "None"
      }
    }
  ],
  "callHierarchy": "Certainly! Below is the call hierarchy for the provided project, structured in a clear, hierarchical format. The main entry point is highlighted, and the primary execution flow is traced through the application.\n\n```\n🚀 app.py (ENTRY POINT)\n├── 📂 parse_arguments() → Parses command-line arguments [app.py]\n│   └── 📂 main(llm_model_name: str, embedding_model_name: str, documents_path: str) → Executes the main logic of the application [app.py]\n│       ├── 📂 check_if_model_is_available(model_name: str) → Ensures the model is available locally [models.py]\n│       ├── 📂 load_documents_into_database(model_name: str, documents_path: str, reload: bool = True) → Loads documents into the Chroma database [document_loader.py]\n│       │   └── 📂 load_documents(path: str) → Loads documents from the specified directory [document_loader.py]\n│       ├── 📂 getStreamingChain(question: str, memory: ConversationBufferMemory, llm: language model, db: database) → Creates a streaming chain for generating responses [llm.py]\n│       │   └── 📂 _combine_documents(docs: list of documents, document_prompt: optional, document_separator: optional) → Combines documents into a single string [llm.py]\n│       └── 📂 get_list_of_models() → Retrieves a list of available models from the Ollama repository [models.py]\n```\n\n### Explanation:\n\n1. **Entry Point**:\n   - The main entry point is `app.py`, specifically the `main` function. This function is called after parsing the command-line arguments.\n\n2. **Execution Flow**:\n   - The `main` function in `app.py` is the primary execution path.\n   - It first checks if the specified model is available locally using `check_if_model_is_available` from `models.py`.\n   - It then loads the documents into the Chroma database using `load_documents_into_database` from `document_loader.py`.\n   - After loading the documents, it creates a streaming chain for generating responses using `getStreamingChain` from `llm.py`.\n   - The `_combine_documents` function from `llm.py` is used to combine the documents into a single string, which is then passed to the streaming chain.\n\n3. **Cross-File Calls**:\n   - `app.py` calls `models.py` to check if the model is available.\n   - `app.py` calls `document_loader.py` to load documents into the database.\n   - `app.py` calls `llm.py` to create a streaming chain for generating responses.\n\n4. **Module Dependencies**:\n   - `app.py` depends on `models.py`, `document_loader.py`, and `llm.py`.\n   - `document_loader.py` depends on `langchain_community`, `langchain_core`, `langchain_ollama`, and `langchain`.\n   - `llm.py` depends on `langchain` and `langchain_core`.\n   - `models.py` depends on `ollama` and `tqdm`.\n\n### Visual Mapping:\n\n```\n🚀 app.py (ENTRY POINT)\n├── 📂 parse_arguments() → Parses command-line arguments [app.py]\n│   └── 📂 main(llm_model_name: str, embedding_model_name: str, documents_path: str) → Executes the main logic of the application [app.py]\n│       ├── 📂 check_if_model_is_available(model_name: str) → Ensures the model is available locally [models.py]\n│       ├── 📂 load_documents_into_database(model_name: str, documents_path: str, reload: bool = True) → Loads documents into the Chroma database [document_loader.py]\n│       │   └── 📂 load_documents(path: str) → Loads documents from the specified directory [document_loader.py]\n│       ├── 📂 getStreamingChain(question: str, memory: ConversationBufferMemory, llm: language model, db: database) → Creates a streaming chain for generating responses [llm.py]\n│       │   └── 📂 _combine_documents(docs: list of documents, document_prompt: optional, document_separator: optional) → Combines documents into a single string [llm.py]\n│       └── 📂 get_list_of_models() → Retrieves a list of available models from the Ollama repository [models.py]\n```\n\nThis structure clearly shows the flow of the application from the entry point through the various files and functions, highlighting the dependencies and significant function calls between different modules.",
  "fileAnalysis": [
    {
      "path": "app.py",
      "analysis": "### 1. Main purpose and responsibilities\nThe `app.py` file is the main entry point of the application, responsible for setting up the environment, loading models, and handling user interactions. It uses LangChain and Ollama for language model operations and integrates with a document loader to provide a question-answering system based on retrieved documents.\n\n### 2. Key functions and their purposes\n- **`main(llm_model_name: str, embedding_model_name: str, documents_path: str) -> None`**:\n  - **Inputs**:\n    - `llm_model_name` (str): The name of the language model to use.\n    - `embedding_model_name` (str): The name of the embedding model to use.\n    - `documents_path` (str): The path to the directory containing documents to load.\n  - **Processing**:\n    - Checks if the specified models are available and attempts to download them if not.\n    - Loads documents into a database using the specified embedding model.\n    - Initializes a chat chain using the language model and the database.\n    - Enters an infinite loop where it prompts the user for questions and processes them using the chat chain.\n  - **Output**: None\n\n- **`parse_arguments() -> argparse.Namespace`**:\n  - **Inputs**: None\n  - **Processing**: Parses command-line arguments to configure the application.\n  - **Output**: `argparse.Namespace` object containing the parsed arguments.\n\n### 3. Important interactions with other parts of the system\n- **`models.py`**: `check_if_model_is_available` is called to check if the specified models are available.\n- **`document_loader.py`**: `load_documents_into_database` is used to load documents into a database.\n- **`llm.py`**: `ChatOllama` and `getChatChain` are used to initialize the language model and the chat chain.\n- **`argparse`**: `parse_arguments` is used to handle command-line arguments.\n\n### 4. Notable features or patterns\n- **Error Handling**: The `main` function includes error handling for model availability and document loading, ensuring the application can gracefully handle issues.\n- **Command-Line Interface**: The use of `argparse` for command-line argument parsing allows for flexible configuration of the application.\n- **Infinite Loop**: The `main` function runs in an infinite loop, allowing for continuous interaction with the user until the user decides to exit.\n\n### Overall\nThe `app.py` file serves as the main driver of the application, setting up the environment, loading necessary models, and providing an interactive interface for users to ask questions based on the provided documents. It leverages other modules for model management, document loading, and chat functionality, ensuring a modular and maintainable design."
    },
    {
      "path": "document_loader.py",
      "analysis": "### 1. Main purpose and responsibilities\nThe `document_loader.py` file is responsible for loading documents from a specified directory and preparing them for storage in a vector database using the Chroma library. It handles the process of splitting the text into manageable chunks and creating embeddings for these chunks using an external model. This file plays a crucial role in the data preprocessing pipeline, ensuring that the documents are ready for further processing such as querying or indexing.\n\n### 2. Key functions and their purposes\n- **`load_documents_into_database(model_name: str, documents_path: str, reload: bool = True) -> Chroma`**:\n  - **Inputs**:\n    - `model_name` (str): The name of the model used for generating embeddings.\n    - `documents_path` (str): The path to the directory containing the documents to be loaded.\n    - `reload` (bool, optional): A flag indicating whether to reload the documents. Defaults to `True`.\n  - **Processing**:\n    - Checks if `reload` is `True`. If so, it loads the documents, splits them into chunks, creates embeddings using the specified model, and stores them in the Chroma database.\n    - If `reload` is `False`, it simply loads the documents from the existing Chroma database.\n  - **Output**:\n    - `Chroma`: The Chroma database containing the loaded and embedded documents.\n\n- **`load_documents(path: str) -> List[Document]`**:\n  - **Inputs**:\n    - `path` (str): The path to the directory containing the documents to be loaded.\n  - **Processing**:\n    - Validates if the specified path exists. If not, it raises a `FileNotFoundError`.\n    - Defines loaders for different file types (PDF, Markdown).\n    - Iterates over the supported file types, loads the documents using the appropriate loader, and compiles them into a list.\n  - **Output**:\n    - `List[Document]`: A list of loaded documents.\n\n### 3. Important interactions with other parts of the system\n- **Interaction with `app.py`**: `document_loader.py` is likely called from `app.py` during the initialization of the application to ensure that the documents are preprocessed and stored in the database.\n- **Interaction with `Chroma`**: It interacts with the Chroma vector database to store the processed documents and their embeddings.\n- **Interaction with `OllamaEmbeddings`**: It uses the `OllamaEmbeddings` class to generate embeddings for the document chunks.\n- **Interaction with `RecursiveCharacterTextSplitter`**: It uses this class to split the documents into smaller chunks before storing them.\n\n### 4. Notable features or patterns\n- **Modular Design**: The file is modular, with separate functions for loading documents and storing them in the database. This makes it easier to maintain and extend.\n- **Support for Multiple Document Types**: It supports loading multiple types of documents (PDF, Markdown) by dynamically choosing the appropriate loader based on the file extension.\n- **Chunking and Embedding**: The documents are first split into chunks for better manageability and then embedded using an external model, which is a common approach in information retrieval systems.\n- **Reloading Mechanism**: The `reload` parameter allows for reprocessing the documents, which can be useful for updating the database without having to manually delete and recreate it.\n\n### Overall\nThe `document_loader.py` file is a critical component of the system responsible for preprocessing and storing documents in a structured format. It leverages various libraries and tools to handle different document types, split the text into manageable chunks, and create embeddings for efficient storage and retrieval. Its modular design and support for multiple document types make it flexible and adaptable to different use cases."
    },
    {
      "path": "llm.py",
      "analysis": "### 1. Main purpose and responsibilities\nThe `llm.py` file is responsible for defining and configuring the interaction between a language model (LLM) and the rest of the application. It primarily focuses on creating and managing chains of operations that involve question-answering, document retrieval, and streaming responses. This file is crucial for integrating the LLM with the application's user interface and database.\n\n### 2. Key functions and their purposes\n- **`_combine_documents(docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\") -> str`**:\n  - **Inputs**: \n    - `docs` (List): A list of documents to be combined.\n    - `document_prompt` (PromptTemplate): A template for formatting each document.\n    - `document_separator` (str): A string used to separate document content.\n  - **Processing**: The function formats each document using the provided `document_prompt` and joins them with the specified `document_separator`.\n  - **Output**: A single string containing all documents formatted and separated as specified.\n\n- **`getStreamingChain(question: str, memory, llm, db) -> RunnableLambda`**:\n  - **Inputs**: \n    - `question` (str): The question to be answered.\n    - `memory` (ConversationBufferMemory): The memory object to store the conversation history.\n    - `llm` (LanguageModel): The language model to be used.\n    - `db` (Database): The database to retrieve relevant documents.\n  - **Processing**: \n    - It sets up a chain of operations that includes condensing the question, retrieving relevant documents, and generating an answer using the LLM.\n    - The chain is configured to stream the response to the user.\n  - **Output**: A `RunnableLambda` object representing the chain of operations.\n\n- **`getChatChain(llm, db) -> Callable[[str], None]`**:\n  - **Inputs**: \n    - `llm` (LanguageModel): The language model to be used.\n    - `db` (Database): The database to retrieve relevant documents.\n  - **Processing**: \n    - It sets up a chain of operations that includes condensing the question, retrieving relevant documents, and generating an answer using the LLM.\n    - The chain is configured to handle chat interactions and stream responses.\n  - **Output**: A callable function that takes a question and processes it using the configured chain.\n\n### 3. Important interactions with other parts of the system\n- **Interaction with `document_loader.py`**: The `getStreamingChain` and `getChatChain` functions rely on a database (`db`) to retrieve relevant documents for the question. This interaction is crucial for ensuring that the LLM has access to the necessary context.\n- **Interaction with `ui.py` and `app.py`**: The `getStreamingChain` and `getChatChain` functions are designed to be used in the application's user interface and backend. They provide the logic for handling user questions and generating responses.\n- **Interaction with `models.py`**: The functions use a language model (`llm`) and a conversation memory (`memory`), which are likely defined in `models.py`.\n\n### 4. Notable features or patterns\n- **Use of LangChain**: The file extensively uses LangChain, a library for building applications that use language models. This includes the use of `RunnableLambda`, `RunnablePassthrough`, and `PromptTemplate`.\n- **Streaming Responses**: The `getStreamingChain` function is specifically designed to stream responses, which is useful for real-time interactions.\n- **Memory Management**: The use of `ConversationBufferMemory` ensures that the conversation history is maintained and can be used to improve the context for subsequent questions.\n\n### Overall\nThe `llm.py` file plays a central role in integrating the language model with the rest of the application. It defines and configures chains of operations that handle question-answering, document retrieval, and streaming responses. The file leverages LangChain for its operations and ensures that the conversation history is maintained for better context."
    },
    {
      "path": "models.py",
      "analysis": "### 1. Main purpose and responsibilities\nThe `models.py` file is responsible for managing the availability and retrieval of machine learning models from the Ollama repository. It includes functionalities to check if a model is available locally, retrieve a list of all available models, and ensure that a specific model is downloaded if it is not already present.\n\n### 2. Key functions and their purposes\n- **`__pull_model(name: str) -> None`**:\n  - **Inputs**: \n    - `name` (str): The name of the model to be pulled.\n  - **Processing**: \n    - This function handles the downloading of a model from the Ollama repository. It uses the `ollama.pull` method to download the model in a streaming manner, updating a progress bar for each chunk of data being downloaded. It also manages the closure of progress bars when chunks are fully downloaded.\n  - **Output**: \n    - None (side effects include updating progress bars and downloading the model).\n\n- **`__is_model_available_locally(model_name: str) -> bool`**:\n  - **Inputs**: \n    - `model_name` (str): The name of the model to check.\n  - **Processing**: \n    - This function checks if the specified model is available locally by attempting to call `ollama.show` on the model name. If `ollama.show` succeeds, the model is considered available; otherwise, it raises an `ollama.ResponseError`.\n  - **Output**: \n    - `bool`: Returns `True` if the model is available locally, `False` otherwise.\n\n- **`get_list_of_models() -> list[str]`**:\n  - **Inputs**: \n    - None.\n  - **Processing**: \n    - This function retrieves a list of all available models from the Ollama repository using the `ollama.list()` method, which returns a dictionary containing information about the models. It then extracts and returns the list of model names.\n  - **Output**: \n    - `list[str]`: A list of strings representing the names of the available models.\n\n- **`check_if_model_is_available(model_name: str) -> None`**:\n  - **Inputs**: \n    - `model_name` (str): The name of the model to check.\n  - **Processing**: \n    - This function first checks if the specified model is available locally using `__is_model_available_locally`. If the model is not available, it calls `__pull_model` to download the model from the Ollama repository. If any exceptions occur during these operations, it raises an exception indicating the failure.\n  - **Output**: \n    - None (raises exceptions on failure).\n\n### 3. Important interactions with other parts of the system\n- The `models.py` file interacts with the `ollama` library to manage model downloads and availability checks.\n- It communicates with the Ollama API through methods like `ollama.pull`, `ollama.show`, and `ollama.list`.\n- It provides utility functions that can be used by other parts of the application, such as `app.py` or `ui.py`, to manage models dynamically.\n\n### 4. Notable features or patterns\n- **Progress Tracking**: The `__pull_model` function uses `tqdm` to track the progress of the download, providing a visual indicator of the download status.\n- **Exception Handling**: The `check_if_model_is_available` function includes comprehensive error handling to ensure that any issues during the model retrieval process are properly communicated to the user.\n- **Lazy Loading**: The `__is_model_available_locally` function checks for local availability before attempting to download, ensuring that unnecessary downloads are avoided.\n\n### Overall\nThe `models.py` file plays a crucial role in managing the lifecycle of machine learning models within the application. It ensures that models are available locally and provides tools for downloading them efficiently. The use of progress tracking and robust error handling enhances the reliability and user experience of the application."
    },
    {
      "path": "ui.py",
      "analysis": "### 1. Main purpose and responsibilities\nThe `ui.py` file is responsible for creating the user interface for a Streamlit application that allows users to interact with a local Large Language Model (LLM) using a Retrieval-Augmented Generation (RAG) approach. It handles the selection of models, loading of documents, and the chat interface for querying the model.\n\n### 2. Key functions and their purposes\n- **`st.title(\"Local LLM with RAG 📚\")`**:\n  - **Inputs**: None.\n  - **Processing**: Sets the title of the Streamlit app.\n  - **Output**: None.\n\n- **`st.sidebar.selectbox(\"Select a model:\", st.session_state[\"list_of_models\"])`**:\n  - **Inputs**: None.\n  - **Processing**: Displays a sidebar select box for the user to choose a model from a list stored in `st.session_state`.\n  - **Output**: The selected model.\n\n- **`st.sidebar.text_input(\"Enter the folder path:\", PATH)`**:\n  - **Inputs**: None.\n  - **Processing**: Displays a sidebar text input for the user to enter a folder path.\n  - **Output**: The entered folder path.\n\n- **`st.sidebar.button(\"Index Documents\")`**:\n  - **Inputs**: None.\n  - **Processing**: Displays a sidebar button to trigger the indexing of documents.\n  - **Output**: A boolean indicating if the button was clicked.\n\n- **`st.error(\"The provided path is not a valid directory. Please enter a valid folder path.\")`**:\n  - **Inputs**: None.\n  - **Processing**: Displays an error message if the provided path is not a valid directory.\n  - **Output**: None.\n\n- **`st.info(\"All set to answer questions!\")`**:\n  - **Inputs**: None.\n  - **Processing**: Displays an info message once the documents are indexed.\n  - **Output**: None.\n\n- **`st.warning(\"Please enter a folder path to load documents into the database.\")`**:\n  - **Inputs**: None.\n  - **Processing**: Displays a warning message if no folder path is provided.\n  - **Output**: None.\n\n- **`st.chat_message(message[\"role\"]):`**:\n  - **Inputs**: `message` (dictionary).\n  - **Processing**: Displays a chat message with the specified role (e.g., user, assistant).\n  - **Output**: None.\n\n- **`st.chat_input(\"Question (indexing required)\", disabled=True)`**:\n  - **Inputs**: None.\n  - **Processing**: Displays a chat input field that is disabled until documents are indexed.\n  - **Output**: None.\n\n- **`st.write_stream(stream)`**:\n  - **Inputs**: `stream` (streaming response from the model).\n  - **Processing**: Writes the streaming response to the chat interface.\n  - **Output**: None.\n\n### 3. Important interactions with other parts of the system\n- **`streamlit`**: The application uses Streamlit for creating the user interface.\n- **`document_loader.py`**: The `load_documents_into_database` function is called to load documents into a database.\n- **`llm.py`**: The `getStreamingChain` function is used to create a streaming chain for generating responses.\n- **`models.py`**: The `get_list_of_models` function is used to retrieve a list of available models.\n- **`langchain_ollama`**: The `ChatOllama` class is used to interact with the LLM.\n\n### 4. Notable features or patterns\n- **Session State**: The file extensively uses `st.session_state` to store and manage application state, such as the selected model, database, and chat messages.\n- **Streaming Responses**: The chat interface supports streaming responses from the model, providing a more natural and interactive experience.\n- **Error Handling**: The file includes basic error handling, such as checking if the provided folder path is valid.\n\n### Overall\nThe `ui.py` file serves as the primary interface for users to interact with a local LLM using a RAG approach. It manages the selection of models, loading of documents, and the chat interface for querying the model. The file leverages Streamlit for the user interface and integrates with other components to provide a seamless and interactive experience."
    }
  ],
  "summary": "This project is a local implementation of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) for answering questions based on sample PDFs using Ollama.\n\n1. **Main purpose and functionality**: The project aims to run local LLMs to perform RAG, providing a command-line interface and a Streamlit web UI for interaction.\n\n2. **Tech stack and architecture**: The tech stack includes Python, Langchain, Ollama, Chroma, PyPDF2, Streamlit, and UV. The architecture follows a modular design with clear separation of concerns, including data loading, model management, and main application logic.\n\n3. **Key components and their interactions**: The main components are `app.py`, `document_loader.py`, `llm.py`, and `models.py`. `app.py` orchestrates the process, `document_loader.py` handles document loading, `llm.py` manages model interactions, and `models.py` ensures model availability. The `ui.py` script provides a Streamlit-based UI.\n\n4. **Notable features**: Key features include the use of Ollama for local LLMs, Chroma for vector database operations, and Streamlit for a user-friendly web interface. The project also includes a command-line dependency and a static document directory for testing.\n\n5. **Code organization and structure**: The code is organized into modules and scripts, with clear imports and dependencies. The file tree shows a well-structured project with separate directories for images, research documents, and other assets.\n\nOverall, this project provides a comprehensive solution for running local LLMs with RAG, leveraging modern Python libraries and frameworks."
}