{
  "repository": {
    "name": "local-LLM-with-RAG",
    "description": "Running local Language Language Models (LLM) to perform Retrieval-Augmented Generation (RAG)",
    "language": "Python"
  },
  "fileMetadata": [
    {
      "path": "app.py",
      "metadata": "{\n  \"name\": \"app.py\",\n  \"path\": \"app.py\",\n  \"imports\": [\n    \"from langchain_ollama import ChatOllama\",\n    \"from models import check_if_model_is_available\",\n    \"from document_loader import load_documents_into_database\",\n    \"import argparse\",\n    \"import sys\",\n    \"from llm import getChatChain\"\n  ],\n  \"mainPurpose\": \"To run a local LLM with RAG using the Ollama framework, allowing users to interact with documents through a chat interface.\",\n  \"type\": \"script\",\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"purpose\": \"Main function to check model availability, load documents, and handle user interactions.\",\n      \"input\": \"llm_model_name (str), embedding_model_name (str), documents_path (str)\",\n      \"output\": \"None\"\n    },\n    {\n      \"name\": \"parse_arguments\",\n      \"purpose\": \"To parse command-line arguments for model names and document path.\",\n      \"input\": \"\",\n      \"output\": \"argparse.Namespace\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"langchain_ollama\",\n    \"models\",\n    \"document_loader\",\n    \"argparse\",\n    \"sys\",\n    \"llm\"\n  ],\n  \"finalReturnType(s)\": \"None\"\n}"
    },
    {
      "path": "document_loader.py",
      "metadata": "{\n  \"name\": \"document_loader.py\",\n  \"path\": \"document_loader.py\",\n  \"imports\": [\n    \"from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\",\n    \"import os\",\n    \"from typing import List\",\n    \"from langchain_core.documents import Document\",\n    \"from langchain_ollama import OllamaEmbeddings\",\n    \"from langchain_community.vectorstores import Chroma\",\n    \"from langchain.text_splitter import RecursiveCharacterTextSplitter\"\n  ],\n  \"mainPurpose\": \"To load documents from a specified directory into a Chroma database after processing them.\",\n  \"type\": \"module\",\n  \"functions\": [\n    {\n      \"name\": \"load_documents_into_database\",\n      \"purpose\": \"Loads documents from a specified directory into the Chroma database after splitting the text into chunks.\",\n      \"input\": \"model_name (str), documents_path (str), reload (bool)\",\n      \"output\": \"Chroma\"\n    },\n    {\n      \"name\": \"load_documents\",\n      \"purpose\": \"Loads documents from the specified directory path, supporting PDF and Markdown formats.\",\n      \"input\": \"path (str)\",\n      \"output\": \"List[Document]\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"langchain_community\",\n    \"langchain_core\",\n    \"langchain_ollama\",\n    \"langchain_community.vectorstores\"\n  ],\n  \"finalReturnType(s)\": \"Chroma, List[Document]\"\n}"
    },
    {
      "path": "llm.py",
      "metadata": "{\n  \"name\": \"llm.py\",\n  \"path\": \"llm.py\",\n  \"imports\": [\n    \"itemgetter from operator\",\n    \"StreamingStdOutCallbackHandler from langchain.callbacks.streaming_stdout\",\n    \"ConversationBufferMemory from langchain.memory\",\n    \"ChatPromptTemplate from langchain_core.prompts\",\n    \"RunnableLambda from langchain_core.runnables\",\n    \"RunnablePassthrough from langchain_core.runnables\",\n    \"get_buffer_string from langchain_core.messages\",\n    \"format_document from langchain_core.prompts\",\n    \"PromptTemplate from langchain.prompts.prompt\"\n  ],\n  \"mainPurpose\": \"To define functions for processing questions and retrieving answers based on conversation history and research documents using LangChain.\",\n  \"type\": \"Python module\",\n  \"functions\": [\n    {\n      \"name\": \"_combine_documents\",\n      \"purpose\": \"Combines multiple documents into a single string formatted with a specified template.\",\n      \"input\": \"docs (list), document_prompt (PromptTemplate), document_separator (str)\",\n      \"output\": \"str\"\n    },\n    {\n      \"name\": \"getStreamingChain\",\n      \"purpose\": \"Creates a streaming chain to process a question and retrieve an answer using memory and a database.\",\n      \"input\": \"question (str), memory, llm, db\",\n      \"output\": \"stream\"\n    },\n    {\n      \"name\": \"getChatChain\",\n      \"purpose\": \"Creates a chat chain to process a question and retrieve an answer using memory and a database.\",\n      \"input\": \"llm, db\",\n      \"output\": \"function chat(question: str)\"\n    },\n    {\n      \"name\": \"chat\",\n      \"purpose\": \"Processes a question and retrieves an answer, saving the context to memory.\",\n      \"input\": \"question (str)\",\n      \"output\": \"None\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"langchain\",\n    \"langchain_core\"\n  ],\n  \"finalReturnType(s)\": \"stream, function\"\n}"
    },
    {
      "path": "models.py",
      "metadata": "{\n  \"name\": \"models.py\",\n  \"path\": \"models.py\",\n  \"imports\": [\n    \"ollama\",\n    \"tqdm\"\n  ],\n  \"mainPurpose\": \"Manage and retrieve machine learning models from the Ollama repository.\",\n  \"type\": \"module\",\n  \"functions\": [\n    {\n      \"name\": \"__pull_model\",\n      \"purpose\": \"Pulls a model from the Ollama repository and displays progress.\",\n      \"input\": \"name: str\",\n      \"output\": \"None\"\n    },\n    {\n      \"name\": \"__is_model_available_locally\",\n      \"purpose\": \"Checks if a specified model is available locally.\",\n      \"input\": \"model_name: str\",\n      \"output\": \"bool\"\n    },\n    {\n      \"name\": \"get_list_of_models\",\n      \"purpose\": \"Retrieves a list of available models from the Ollama repository.\",\n      \"input\": \"\",\n      \"output\": \"list[str]\"\n    },\n    {\n      \"name\": \"check_if_model_is_available\",\n      \"purpose\": \"Ensures that a specified model is available locally, pulling it if necessary.\",\n      \"input\": \"model_name: str\",\n      \"output\": \"None\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"ollama\",\n    \"tqdm\"\n  ],\n  \"finalReturnType(s)\": \"None, bool, list[str]\"\n}"
    },
    {
      "path": "pyproject.toml",
      "metadata": "{\n  \"name\": \"local-llm-with-rag\",\n  \"path\": \"pyproject.toml\",\n  \"imports\": [],\n  \"mainPurpose\": \"Configuration file for a Python project specifying project metadata and dependencies.\",\n  \"type\": \"configuration\",\n  \"functions\": [],\n  \"exports\": [],\n  \"dependencies\": [\n    \"chromadb==0.6.3\",\n    \"langchain==0.3.18\",\n    \"langchain-community==0.3.17\",\n    \"langchain-ollama>=0.2.3\",\n    \"ollama==0.4.7\",\n    \"pypdf==5.3.0\",\n    \"streamlit==1.42.0\",\n    \"tqdm==4.67.1\",\n    \"watchdog==6.0.0\"\n  ],\n  \"finalReturnType(s)\": \"\"\n}"
    },
    {
      "path": "ui.py",
      "metadata": "{\n  \"name\": \"ui.py\",\n  \"path\": \"ui.py\",\n  \"imports\": [\n    \"streamlit as st\",\n    \"os\",\n    \"langchain_ollama.ChatOllama\",\n    \"document_loader.load_documents_into_database\",\n    \"models.get_list_of_models\",\n    \"llm.getStreamingChain\"\n  ],\n  \"mainPurpose\": \"To create a user interface for interacting with a local LLM using a retrieval-augmented generation (RAG) approach.\",\n  \"type\": \"Streamlit application\",\n  \"functions\": [\n    {\n      \"name\": \"get_list_of_models\",\n      \"purpose\": \"Retrieves a list of available models for selection.\",\n      \"input\": \"\",\n      \"output\": \"List of model names.\"\n    },\n    {\n      \"name\": \"load_documents_into_database\",\n      \"purpose\": \"Loads documents from a specified folder into a database after creating embeddings.\",\n      \"input\": \"Embedding model name and folder path.\",\n      \"output\": \"Database object containing indexed documents.\"\n    },\n    {\n      \"name\": \"getStreamingChain\",\n      \"purpose\": \"Generates a streaming response from the LLM based on user input and chat history.\",\n      \"input\": \"User prompt, chat messages, LLM instance, and database.\",\n      \"output\": \"Streaming response from the assistant.\"\n    }\n  ],\n  \"exports\": [],\n  \"dependencies\": [\n    \"streamlit\",\n    \"os\",\n    \"langchain_ollama\",\n    \"document_loader\",\n    \"models\",\n    \"llm\"\n  ],\n  \"finalReturnType(s)\": \"None\"\n}"
    }
  ],
  "callHierarchy": "Here's a structured call hierarchy for the **local-LLM-with-RAG** project, outlining the main execution path, important function calls, and dependencies between modules.\n\n### Call Hierarchy\n\n```plaintext\n📁 main() → None [app.py]\n├─ 🔷 check_if_model_is_available(model_name: str) → None [models.py]\n├─ 🔷 load_documents_into_database(model_name: str, documents_path: str, reload: bool) → Chroma [document_loader.py]\n│   ├─ 🟣 load_documents(path: str) → List[Document] [document_loader.py]\n│   └─ 🟠 Chroma (database object) [document_loader.py]\n└─ 🔶 getChatChain(llm, db) → function chat(question: str) [llm.py]\n    ├─ 🟢 chat(question: str) → None [llm.py]\n    └─ 🔴 getStreamingChain(question: str, memory, llm, db) → stream [llm.py]\n```\n\n### Entry Point File\n- **`app.py`**: This is the main entry point of the application where the execution begins.\n\n### Main Execution Flow\n1. **`main()`** in `app.py` is called, which serves as the entry point.\n2. **Model Availability Check**:\n   - Calls **`check_if_model_is_available(model_name)`** from `models.py` to ensure the specified model is available locally.\n3. **Document Loading**:\n   - Calls **`load_documents_into_database(model_name, documents_path, reload)`** from `document_loader.py` to load documents into the Chroma database.\n   - This function internally calls **`load_documents(path)`**, which loads documents from the specified directory.\n4. **LLM Interaction**:\n   - Calls **`getChatChain(llm, db)`** from `llm.py` to create a chat chain for processing user queries.\n   - This function returns a `chat` function that can be invoked to answer user questions.\n5. **Chat Processing**:\n   - The `chat(question)` function processes user input and retrieves answers based on the conversation history and loaded documents.\n   - Optionally, it may call **`getStreamingChain(question, memory, llm, db)`** for streaming responses.\n\n### Important Function Calls Between Files\n- **`app.py`**:\n  - Calls functions from `models.py`, `document_loader.py`, and `llm.py`.\n- **`document_loader.py`**:\n  - Calls **`load_documents(path)`** to load documents.\n- **`llm.py`**:\n  - Calls **`chat(question)`** and **`getStreamingChain(question, memory, llm, db)`** for processing queries.\n\n### Dependencies Between Modules\n- **`app.py`**:\n  - Depends on: `models`, `document_loader`, `llm`, `langchain_ollama`, `argparse`, `sys`\n  \n- **`document_loader.py`**:\n  - Depends on: `langchain_community`, `langchain_core`, `langchain_ollama`, `langchain_community.vectorstores`\n  \n- **`llm.py`**:\n  - Depends on: `langchain`, `langchain_core`\n  \n- **`models.py`**:\n  - Depends on: `ollama`, `tqdm`\n  \n- **`ui.py`**:\n  - Depends on: `streamlit`, `os`, `langchain_ollama`, `document_loader`, `models`, `llm`\n\nThis structured representation provides a clear overview of how the application flows from the entry point through various files and functions, highlighting the important interactions and dependencies between different components of the project."
}